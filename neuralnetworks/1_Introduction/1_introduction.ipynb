{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks introduction\n",
    "## I) Concept and Theory\n",
    "\n",
    "Neural Networks (NNs) are a subset of machine learning models inspired by the structure and function of the human brain. They are composed of interconnected nodes or \"neurons,\" which are organized into layers. These networks are particularly powerful for capturing complex patterns in data and are widely used in applications such as image recognition, natural language processing, and predictive analytics.\n",
    "\n",
    "### a) Perceptron Definition\n",
    "\n",
    "A neuron is the fundamental unit of a neural network. Each neuron receives one or more inputs, processes them, and produces an output. Mathematically, a neuron can be represented as:\n",
    "\n",
    "$$\n",
    "y = f\\left(\\sum_{i=1}^{n} w_i x_i + b\\right)\n",
    "$$\n",
    "\n",
    "- $( x_i )$: Input features\n",
    "- $( w_i )$: Weights associated with each input\n",
    "- $( b )$: Bias term\n",
    "- $( f )$: Activation function\n",
    "- $( y )$: Output of the neuron\n",
    "\n",
    "The activation function have properties like : Non-Linearity, Differentiability, Monotonicity ... to ensure learning and capturing complex data. For simplification \n",
    "<br><br>\n",
    "The most simple form of neural networks is the single Layer Perceptron looking like that :\n",
    "<br><br>\n",
    "<img src=\"static/SingleLayerPerceptron.png\" height=\"40%\" width=\"40%\">\n",
    "<br>\n",
    "Let's do a simple example following the picture (we took arbitrary numbers and functions) :\n",
    " - Input vector : $x=(x_1,x_2,x_3)=(1,2,3)$\n",
    " - Weights : $W=(w_1,w_2,w_3)=(0.5,0.3,2)$\n",
    " - Bias : $b=0$ \n",
    " - Activation function : $f=f(x) = \\frac{1}{1 + e^{-x}}$ (the sigmoid function)\n",
    "\n",
    "To compute $a_1=f(x_1*w_1+x_2*w_2+x_3*w_3)=f(7.1)\\approx 0.99$.\n",
    "<br>\n",
    "This is called the <b>Feed Forwad Process</b>.\n",
    "<br><br>\n",
    "\n",
    "### b) Training a Perceptron\n",
    "\n",
    "Training a neural network involves adjusting the weights and biases to minimize the difference between the predicted output and the actual target.\n",
    "Let's define the Loss Function, this function quantifies the error in the predictions. Common loss functions include Mean Squared Error (MSE) $(\\frac{1}{N} \\sum_{i=1}^{N} \\left( \\hat{y}_i - y_i \\right)^2)$ for regression tasks and Cross-Entropy Loss $( -\\frac{1}{N} \\sum_{i=1}^{N} \\sum_{c=1}^{C} y_{i,c} \\log(\\hat{y}_{i,c})\n",
    ")$ for classification tasks. Goal is to minimize this functions accross all the observations.\n",
    "<br><br>\n",
    "Backpropagation is the algorithm used to update the network's weights. It involves computing the gradient of the loss function with respect to each weight using the chain rule, then updating the weights in the direction that reduces the loss.\n",
    "\n",
    "Mathematically, the weight update rule can be expressed as:\n",
    "\n",
    "$w_{ij} \\leftarrow w_{ij} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial w_{ij}}$\n",
    "\n",
    "Where:\n",
    "- $( w_{ij} )$ is the weight connecting neuron \\( j \\) to neuron \\( i \\)\n",
    "- $( \\eta )$ is the learning rate\n",
    "- $( \\mathcal{L} )$ is the loss function\n",
    "\n",
    "Gradient Descent is an optimization algorithm used to minimize the loss function. In each iteration, the algorithm updates the weights by moving them in the direction opposite to the gradient.\n",
    "<br><br>\n",
    "Let's do a simple example of backpropagation where :\n",
    " - Input vector : $x=(x_1,x_2,x_3)=(1,2,3)$\n",
    " - Weights : $W=(w_1,w_2,w_3)=(0.5,0.3,2)$\n",
    " - Target : $y=(0.2)$ \n",
    " - Activation function : $f=f(x) = \\frac{1}{1 + e^{-x}}$ (the sigmoid function)\n",
    " - Loss function : $\\mathcal{L} =0.5*(\\hat{y}-y)^2$\n",
    " - Learning Rate : $( \\eta )=0.1$\n",
    "\n",
    " <br>\n",
    "\n",
    " 1. Compute Forward Pass : $\\hat{y}=0.99$ (we use the approximation from a) )\n",
    " 2. Compute the Loss : $\\mathcal{L} = 0.5 (0.99-0.2)^2=0.31205$\n",
    " 3. Compute the Gradient of the Loss : $\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}}=\\hat{y}-y=0.79$\n",
    " 4. Compute the Gradient of the the Weighted Sum : $z=\\sum_{i=1}^{n} w_i x_i$ and  $f'(x)=\\frac{e^{-x}}{(1 + e^{-x})^2}$ so $f'(7.1)=0.000823745$\n",
    " 5. Compute the Gradient of the Loss for each weights : <br>\n",
    "\n",
    " Use the chain rule to compute the gradient of the loss with respect to each weight : $\\frac{\\partial \\mathcal{L}}{\\partial w_{i}}=\\frac{\\partial \\mathcal{L}}{\\partial \\hat{y}}*\\frac{\\partial \\hat{y}}{\\partial z}*\\frac{\\partial z}{\\partial w_{i}}$ and $\\frac{\\partial z}{\\partial w_{i}}=x_i$ so we got $\\frac{\\partial \\mathcal{L}}{\\partial w_{i}}=(\\hat{y}-y)*f'(z)*x_i$\n",
    " <br>\n",
    " $\\frac{\\partial \\mathcal{L}}{\\partial W}=(0.00065075855,0.0013015171,0.00195227565)$\n",
    "\n",
    " 6. Update the Weights : Using the formula $w_{ij} \\leftarrow w_{ij} - \\eta \\frac{\\partial \\mathcal{L}}{\\partial w_{ij}}$ -> $W_{new}=(0.499934924145,0.29986984829,0.199804772435)$\n",
    "\n",
    "You can iterate these steps; note that generally, we stop the iterations after a certain number of steps or when the loss function reaches a specified value. Additionally, for efficiency in terms of time and computational resources, we often use optimizers that implement stochastic gradient descent.\n",
    "<br><br>\n",
    "\n",
    "## II) Building a Perceptron from scratch in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted output before training: [0.69243671]\n",
      "\n",
      "Epoch 0, Loss: 0.2397\n",
      "Epoch 10, Loss: 0.2095\n",
      "Epoch 20, Loss: 0.1800\n",
      "Epoch 30, Loss: 0.1526\n",
      "Epoch 40, Loss: 0.1286\n",
      "Epoch 50, Loss: 0.1082\n",
      "Epoch 60, Loss: 0.0913\n",
      "Epoch 70, Loss: 0.0776\n",
      "Epoch 80, Loss: 0.0666\n",
      "Epoch 90, Loss: 0.0576\n",
      "\n",
      "Predicted output after training : [0.31738198]\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Activation function (Sigmoid) and its derivative\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# Loss function (Mean Squared Error)\n",
    "def mean_squared_error(y_true, y_pred):\n",
    "    return 0.5 * np.mean((y_true - y_pred) ** 2)\n",
    "\n",
    "# Single perceptron class\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size, learning_rate=0.01):\n",
    "        self.weights = np.random.rand(input_size)\n",
    "        self.bias = np.random.rand(1) \n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = np.dot(x, self.weights) + self.bias\n",
    "        return sigmoid(z), z\n",
    "\n",
    "    def backward(self, x, y_true, y_pred, z):\n",
    "        # Compute the gradients\n",
    "        loss_derivative = y_pred - y_true\n",
    "        z_derivative = sigmoid_derivative(z)\n",
    "        \n",
    "        # Gradient with respect to weights and bias\n",
    "        weight_gradients = loss_derivative * z_derivative * x\n",
    "        bias_gradient = loss_derivative * z_derivative\n",
    "\n",
    "        # Update weights and bias\n",
    "        self.weights -= self.learning_rate * weight_gradients\n",
    "        self.bias -= self.learning_rate * bias_gradient\n",
    "\n",
    "    def train(self, x, y_true):\n",
    "        # Forward pass\n",
    "        y_pred, z = self.forward(x)\n",
    "        \n",
    "        # Compute loss (for monitoring purposes)\n",
    "        loss = mean_squared_error(y_true, y_pred)\n",
    "        \n",
    "        # Backward pass (update weights and bias)\n",
    "        self.backward(x, y_true, y_pred, z)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Input data (single sample with 3 features)\n",
    "    x = np.array([0.5, 0.3, 0.2])\n",
    "    \n",
    "    # True label\n",
    "    y_true = np.array([0])\n",
    "    \n",
    "    # Create perceptron\n",
    "    perceptron = Perceptron(input_size=3, learning_rate=0.1)\n",
    "    y_pred, _ = perceptron.forward(x)\n",
    "    print(f\"Predicted output before training: {y_pred}\\n\")\n",
    "    # Train the perceptron with the sample\n",
    "    for epoch in range(100):  # Train for 100 epochs\n",
    "        loss = perceptron.train(x, y_true)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss:.4f}\")\n",
    "\n",
    "    # Test the perceptron after training\n",
    "    y_pred, _ = perceptron.forward(x)\n",
    "    print(f\"\\nPredicted output after training : {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III) Other definitions\n",
    "\n",
    "Some definitions:\n",
    "\n",
    "**Input Layer**:\n",
    "- The input layer is where data is fed into the model, typically from sources like CSV files or images, audios... This layer simply passes the data without any processing, making it the only visible layer in the neural network architecture.\n",
    "\n",
    "**Hidden Layers**:\n",
    "- Hidden layers are the core of deep learning. These intermediate layers perform computations to extract features from the data. Multiple interconnected hidden layers can identify various features at different levels of complexity. For instance, in image processing, early hidden layers might detect edges and shapes, while later layers might recognize entire objects like cars or buildings.\n",
    "\n",
    "**Output Layer**:\n",
    "- The output layer receives input from the preceding hidden layers and produces the final prediction based on the model's training. This layer is crucial as it provides the final result. In classification or regression models, the output layer usually consists of a single node, but its structure can vary depending on the specific problem and model design.\n",
    "<br><br>\n",
    "<img src=\"static/Deep-Neural-Network-architecture.ppm\" height=\"40%\" width=\"40%\" />\n",
    "\n",
    "**Epochs**: \n",
    "- An epoch is one complete pass through the entire training dataset. Multiple epochs allow the model to learn better, but too many can cause overfitting.\n",
    "\n",
    "**Batch Size**: \n",
    "- The number of data points the model processes before updating its parameters. Smaller batches can lead to noisy learning, while larger batches make learning slower but more stable.Together, epochs and batch size help balance the efficiency and effectiveness of training a model.\n",
    "\n",
    "**Optimizer**:\n",
    "- An optimizer in neural networks is an algorithm that adjusts the model's weights and biases during training to minimize the loss function, improving the model's accuracy. Common optimizers include Stochastic Gradient Descent (SGD) and Adam.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NN",
   "language": "python",
   "name": "nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
